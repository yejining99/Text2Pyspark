"""
Lang2SQL Streamlit ì• í”Œë¦¬ì¼€ì´ì…˜.

ìì—°ì–´ë¡œ ì…ë ¥ëœ ì§ˆë¬¸ì„ SQL ì¿¼ë¦¬ë¡œ ë³€í™˜í•˜ê³ ,
ClickHouse ë°ì´í„°ë² ì´ìŠ¤ì— ì‹¤í–‰í•œ ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
"""

import sys
import os

# import ê²½ë¡œ ë¬¸ì œ í•´ê²°: í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ sys.pathì— ì¶”ê°€
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)  # interfaceì˜ ìƒìœ„ ë””ë ‰í† ë¦¬ (lang2sql)
if project_root not in sys.path:
    sys.path.insert(0, project_root)

import re
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë”© (ìµœìš°ì„  ì‹¤í–‰)
load_dotenv()

import streamlit as st
import time
import json
from langchain.chains.sql_database.prompt import SQL_PROMPTS
from langchain_core.messages import AIMessage, HumanMessage

# from db_utils import get_db_connector
# from db_utils.base_connector import BaseConnector
# from infra.db.connect_db import ConnectDB
from viz.display_chart import DisplayChart
from engine.query_executor import execute_query as execute_query_common
from llm_utils.llm_response_parser import LLMResponseParser
from infra.observability.token_usage import TokenUtils
from llm_utils.graph_utils.enriched_graph import builder as enriched_builder
from llm_utils.graph_utils.basic_graph import builder
from llm_utils.graph_utils.base import (
    GET_TABLE_INFO,
    PROFILE_EXTRACTION, 
    CONTEXT_ENRICHMENT,
    QUERY_MAKER
)


TITLE = "âš¡ ì‹ ìœ„í—˜ë¥  ì¼ì›í™”í…Œì´ë¸” Text2Pyspark"
DEFAULT_QUERY = "ë‚œì²­(ë…¸ë…„ë‚œì²­ì œì™¸) ì—°ê°„í™˜ììˆ˜ë¥¼ ì‹¤ì†ë°ì´í„°ì—ì„œ CYë³„, ì„±, ì—°ë ¹ 5ì„¸ë‹¨ìœ„ë¡œ ì§‘ê³„í•˜ëŠ” ì¿¼ë¦¬"
SIDEBAR_OPTIONS = {
    "show_token_usage": "Show Token Usage",
    "show_result_description": "Show Result Description",
    "show_sql": "Show SQL",
    "show_question_reinterpreted_by_ai": "Show User Question Reinterpreted by AI",
    "show_referenced_tables": "Show List of Referenced Tables",
    "show_table": "Show Table",
    "show_chart": "Show Chart",
}


def get_node_display_name(node_name: str) -> str:
    """ë…¸ë“œ ì´ë¦„ì„ ì‚¬ìš©ì ì¹œí™”ì ì¸ í•œêµ­ì–´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    node_names = {
        GET_TABLE_INFO: "ğŸ“‹ í…Œì´ë¸” ì •ë³´ ê²€ìƒ‰",
        PROFILE_EXTRACTION: "ğŸ” ì§ˆë¬¸ í”„ë¡œíŒŒì¼ ì¶”ì¶œ",
        CONTEXT_ENRICHMENT: "ğŸ’¡ ì»¨í…ìŠ¤íŠ¸ ë³´ê°•",
        QUERY_MAKER: "âš¡ SQL ì¿¼ë¦¬ ìƒì„±"
    }
    return node_names.get(node_name, node_name)


def display_node_status(status_container, current_node: str, progress: float, total_nodes: int):
    """í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ë…¸ë“œ ìƒíƒœë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."""
    with status_container:
        st.markdown("### ğŸ”„ ì‹¤í–‰ ìƒíƒœ")
        
        # ì§„í–‰ë¥  ë°”
        st.progress(progress)
        st.write(f"ì§„í–‰ë¥ : {progress:.1%} ({int(progress * total_nodes)}/{total_nodes} ë…¸ë“œ ì™„ë£Œ)")
        
        # í˜„ì¬ ì‹¤í–‰ ì¤‘ì¸ ë…¸ë“œ
        if current_node:
            st.markdown(f"**í˜„ì¬ ì‹¤í–‰ ì¤‘:** {get_node_display_name(current_node)}")


def display_node_result(results_container, node_name: str, input_data: dict, output_data: dict, execution_time: float):
    """ë…¸ë“œ ì‹¤í–‰ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."""
    with results_container:
        # ë…¸ë“œ ìƒíƒœì— ë”°ë¥¸ ì•„ì´ì½˜
        status_icon = "âœ…"
        if execution_time > 5:
            status_icon = "âš ï¸"  # ì‹¤í–‰ ì‹œê°„ì´ ê¸¸ë©´ ê²½ê³ 
        
        with st.expander(f"{status_icon} {get_node_display_name(node_name)} - {execution_time:.2f}ì´ˆ", expanded=True):
            
            # íƒ­ìœ¼ë¡œ ì •ë³´ êµ¬ë¶„
            tab1, tab2, tab3 = st.tabs(["ğŸ“¥ ì…ë ¥", "ğŸ“¤ ì¶œë ¥", "ğŸ” ìƒì„¸"])
            
            with tab1:
                st.markdown("**ì…ë ¥ ë°ì´í„°:**")
                if node_name == GET_TABLE_INFO:
                    st.markdown("ğŸ“ **ì‚¬ìš©ì ì§ˆë¬¸:**")
                    user_msg = input_data.get('messages', [{}])[0]
                    question = user_msg.get('content', 'N/A') if isinstance(user_msg, dict) else getattr(user_msg, 'content', 'N/A')
                    st.code(question, language='text')
                    
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("ê²€ìƒ‰ê¸°", input_data.get('retriever_name', 'N/A'))
                    with col2:
                        st.metric("ìƒìœ„ Nê°œ", input_data.get('top_n', 'N/A'))
                    with col3:
                        st.metric("ë””ë°”ì´ìŠ¤", input_data.get('device', 'N/A'))
                        
                elif node_name == PROFILE_EXTRACTION:
                    st.markdown("ğŸ“ **ë¶„ì„í•  ì§ˆë¬¸:**")
                    user_msg = input_data.get('messages', [{}])[0]
                    question = user_msg.get('content', 'N/A') if isinstance(user_msg, dict) else getattr(user_msg, 'content', 'N/A')
                    st.code(question, language='text')
                    
                elif node_name == CONTEXT_ENRICHMENT:
                    st.markdown("ğŸ” **í”„ë¡œíŒŒì¼ ì •ë³´:**")
                    profile = output_data.get('question_profile', {})  # ì´ì „ ë…¸ë“œ ê²°ê³¼ ì‚¬ìš©
                    if profile:
                        if hasattr(profile, 'model_dump'):
                            st.json(profile.model_dump())
                        else:
                            st.json(profile)
                    
                elif node_name == QUERY_MAKER:
                    st.markdown("ğŸ’­ **ìµœì¢… ì…ë ¥ ì§ˆë¬¸:**")
                    messages = output_data.get('messages', [])
                    if len(messages) > 1:
                        last_message = messages[-2]  # QUERY_MAKER ì§ì „ ë©”ì‹œì§€
                        content = last_message.content if hasattr(last_message, 'content') else str(last_message)
                        st.code(content, language='text')
            
            with tab2:
                st.markdown("**ì¶œë ¥ ê²°ê³¼:**")
                if node_name == GET_TABLE_INFO:
                    tables = output_data.get('searched_tables', {})
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("ê²€ìƒ‰ëœ í…Œì´ë¸” ìˆ˜", len(tables))
                    with col2:
                        try:
                            scores = [float(table.get('score', 0)) for table in tables.values() if table.get('score') != 'N/A']
                            avg_score = sum(scores) / len(scores) if scores else 0
                            st.metric("í‰ê·  ìœ ì‚¬ë„", f"{avg_score:.3f}")
                        except:
                            st.metric("í‰ê·  ìœ ì‚¬ë„", "ê³„ì‚° ë¶ˆê°€")
                    
                    if tables:
                        st.markdown("**ğŸ“‹ ê²€ìƒ‰ëœ í…Œì´ë¸” ëª©ë¡:**")
                        for i, (table_name, table_info) in enumerate(list(tables.items())[:5]):
                            score = table_info.get('score', 'N/A')
                            table_desc = table_info.get('table_description', 'N/A')
                            st.write(f"{i+1}. **{table_name}** (ìœ ì‚¬ë„: {score})")
                            st.write(f"   ğŸ“„ {table_desc}")
                    else:
                        st.warning("ê²€ìƒ‰ëœ í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤. ë²¡í„° DB ë˜ëŠ” ì„ë² ë”© ì„¤ì •ì„ í™•ì¸í•´ì£¼ì„¸ìš”.")
                            
                elif node_name == PROFILE_EXTRACTION:
                    profile = output_data.get('question_profile', {})
                    if profile:
                        st.markdown("**ğŸ·ï¸ ì¶”ì¶œëœ í”„ë¡œíŒŒì¼:**")
                        if hasattr(profile, 'model_dump'):
                            profile_dict = profile.model_dump()
                        else:
                            profile_dict = profile
                        
                        # ì¤‘ìš”í•œ ì†ì„±ë“¤ì„ ë©”íŠ¸ë¦­ìœ¼ë¡œ í‘œì‹œ
                        cols = st.columns(3)
                        for i, (key, value) in enumerate(list(profile_dict.items())[:6]):
                            with cols[i % 3]:
                                st.metric(key.replace('_', ' ').title(), str(value))
                                
                elif node_name == CONTEXT_ENRICHMENT:
                    messages = output_data.get('messages', [])
                    if len(messages) > 1:
                        last_message = messages[-1]
                        content = last_message.content if hasattr(last_message, 'content') else str(last_message)
                        st.markdown("**ğŸ’¡ ë³´ê°•ëœ ì§ˆë¬¸:**")
                        st.code(content, language='text')
                        
                elif node_name == QUERY_MAKER:
                    generated_query = output_data.get('generated_query')
                    if generated_query:
                        query_content = generated_query.content if hasattr(generated_query, 'content') else str(generated_query)
                        
                        # SQLê³¼ í•´ì„ ë¶€ë¶„ ë¶„ë¦¬
                        try:
                            sql = LLMResponseParser.extract_sql(query_content)
                            st.markdown("**ğŸ”§ ìƒì„±ëœ SQL:**")
                            st.code(sql, language="sql")
                            
                            interpretation = LLMResponseParser.extract_interpretation(query_content)
                            if interpretation:
                                st.markdown("**ğŸ“– ì¿¼ë¦¬ í•´ì„:**")
                                st.write(interpretation)
                        except Exception as e:
                            st.code(query_content, language="sql")
            
            with tab3:
                st.markdown("**ìƒì„¸ ì •ë³´:**")
                
                # ì‹¤í–‰ ì‹œê°„ ì •ë³´
                if execution_time < 1:
                    time_status = "ğŸŸ¢ ë¹ ë¦„"
                elif execution_time < 3:
                    time_status = "ğŸŸ¡ ë³´í†µ"
                else:
                    time_status = "ğŸ”´ ëŠë¦¼"
                
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("ì‹¤í–‰ ì‹œê°„", f"{execution_time:.2f}ì´ˆ", delta=time_status)
                    
                with col2:
                    st.metric("ë©”ëª¨ë¦¬ ì‚¬ìš©", "ì¶”ì • ì¤‘...")  # í–¥í›„ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì¶”ê°€ ê°€ëŠ¥
                
                # ë…¸ë“œë³„ ìƒì„¸ ì •ë³´
                if node_name == GET_TABLE_INFO:
                    st.markdown("**ğŸ” ê²€ìƒ‰ ìƒì„¸:**")
                    retriever_info = {
                        "ê²€ìƒ‰ ë°©ì‹": input_data.get('retriever_name', 'N/A'),
                        "ë²¡í„° DB ìœ í˜•": "FAISS",
                        "ì„ë² ë”© ëª¨ë¸": "ì¶”ì • ì¤‘..."
                    }
                    st.json(retriever_info)
                    
                elif node_name == PROFILE_EXTRACTION:
                    st.markdown("**ğŸ¯ í”„ë¡œíŒŒì¼ ì¶”ì¶œ ìƒì„¸:**")
                    st.write("ì§ˆë¬¸ì˜ íŠ¹ì„±ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ SQL íŒ¨í„´ì„ ê²°ì •í•©ë‹ˆë‹¤.")
                    
                elif node_name == CONTEXT_ENRICHMENT:
                    st.markdown("**ğŸ”„ ì»¨í…ìŠ¤íŠ¸ ë³´ê°• ìƒì„¸:**")
                    st.write("í”„ë¡œíŒŒì¼ê³¼ í…Œì´ë¸” ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ì§ˆë¬¸ì„ ë” êµ¬ì²´ì ìœ¼ë¡œ ë§Œë“­ë‹ˆë‹¤.")
                    
                elif node_name == QUERY_MAKER:
                    st.markdown("**âš™ï¸ SQL ìƒì„± ìƒì„¸:**")
                    st.write("LLMì„ ì‚¬ìš©í•˜ì—¬ ìì—°ì–´ ì§ˆë¬¸ì„ SQL ì¿¼ë¦¬ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.")
                
                # ì „ì²´ ìƒíƒœ ì •ë³´ (ë””ë²„ê¹…ìš©)
                with st.expander("ğŸ”§ ë””ë²„ê¹… ì •ë³´ (ì „ì²´ ìƒíƒœ)"):
                    st.json({
                        "input_keys": list(input_data.keys()),
                        "output_keys": list(output_data.keys()),
                        "messages_count": len(output_data.get('messages', [])),
                        "node_name": node_name
                    })


def execute_query_with_monitoring(
    *,
    query: str,
    database_env: str,
    retriever_name: str = "ê¸°ë³¸",
    top_n: int = 5,
    device: str = "cpu",
) -> dict:
    """
    ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ê³¼ í•¨ê»˜ ì¿¼ë¦¬ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤.
    """
    # ê·¸ë˜í”„ ì„ íƒ (ê¸°ë³¸ê°’ì„ Trueë¡œ ì„¤ì •í•˜ì—¬ enriched graph ìš°ì„  ì‚¬ìš©)
    use_enriched = st.session_state.get("use_enriched", True)
    graph = st.session_state.get("graph")
    
    if graph is None:
        st.error("ê·¸ë˜í”„ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return {}
    
    # ëª¨ë‹ˆí„°ë§ UI ì»¨í…Œì´ë„ˆ ìƒì„±
    status_container = st.empty()
    results_container = st.container()
    
    # ì „ì²´ ì‹¤í–‰ ì‹œê°„ ì¶”ì 
    total_start_time = time.time()
    
    # ì…ë ¥ ë°ì´í„° ì¤€ë¹„
    initial_input = {
        "messages": [HumanMessage(content=query)],
        "user_database_env": database_env,
        "best_practice_query": "",
        "retriever_name": retriever_name,
        "top_n": top_n,
        "device": device,
    }
    
    # ë…¸ë“œ ì‹¤í–‰ ì¶”ì ì„ ìœ„í•œ ë³€ìˆ˜
    node_sequence = []
    if use_enriched:
        node_sequence = [GET_TABLE_INFO, PROFILE_EXTRACTION, CONTEXT_ENRICHMENT, QUERY_MAKER]
    else:
        node_sequence = [GET_TABLE_INFO, QUERY_MAKER]
    
    total_nodes = len(node_sequence)
    completed_nodes = 0
    results = {}
    
    try:
        # ìŠ¤íŠ¸ë¦¼ ì‹¤í–‰
        for chunk in graph.stream(initial_input):
            for node_name, node_output in chunk.items():
                if node_name in node_sequence:
                    start_time = time.time()
                    
                    # í˜„ì¬ ë…¸ë“œ ìƒíƒœ í‘œì‹œ
                    progress = completed_nodes / total_nodes
                    display_node_status(status_container, node_name, progress, total_nodes)
                    
                    # ë…¸ë“œ ì‹¤í–‰ ì‹œê°„ ì‹œë®¬ë ˆì´ì…˜ (ì‹¤ì œë¡œëŠ” ì´ë¯¸ ì™„ë£Œëœ ìƒíƒœ)
                    execution_time = time.time() - start_time + 0.5  # ìµœì†Œ 0.5ì´ˆ í‘œì‹œ
                    
                    # ê²°ê³¼ í‘œì‹œ
                    display_node_result(
                        results_container,
                        node_name, 
                        initial_input,
                        node_output,
                        execution_time
                    )
                    
                    completed_nodes += 1
                    results = node_output
                    
                    # UI ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ì§§ì€ ì§€ì—°
                    time.sleep(0.2)
        
        # ì™„ë£Œ ìƒíƒœ í‘œì‹œ
        total_execution_time = time.time() - total_start_time
        with status_container:
            st.markdown("### âœ… ì‹¤í–‰ ì™„ë£Œ!")
            st.success(f"ì „ì²´ ì‹¤í–‰ ì‹œê°„: {total_execution_time:.2f}ì´ˆ")
            st.progress(1.0)
            
            # ì‹¤í–‰ ìš”ì•½ ì •ë³´
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ì´ ë…¸ë“œ ìˆ˜", total_nodes)
            with col2:
                st.metric("í‰ê·  ë…¸ë“œ ì‹¤í–‰ ì‹œê°„", f"{total_execution_time/total_nodes:.2f}ì´ˆ")
            with col3:
                performance = "ğŸŸ¢ ìš°ìˆ˜" if total_execution_time < 10 else "ğŸŸ¡ ë³´í†µ" if total_execution_time < 20 else "ğŸ”´ ëŠë¦¼"
                st.metric("ì„±ëŠ¥", performance)
        
        return results
        
    except Exception as e:
        st.error(f"ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")
        return {}


def execute_query(
    *,
    query: str,
    database_env: str,
    retriever_name: str = "ê¸°ë³¸",
    top_n: int = 5,
    device: str = "cpu",
) -> dict:
    """
    ìì—°ì–´ ì¿¼ë¦¬ë¥¼ SQLë¡œ ë³€í™˜í•˜ê³  ì‹¤í–‰ ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” Lang2SQL ê·¸ë˜í”„ ì¸í„°í˜ì´ìŠ¤ í•¨ìˆ˜ì…ë‹ˆë‹¤.

    ì´ í•¨ìˆ˜ëŠ” ê³µìš© execute_query í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ Lang2SQL íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
    Streamlit ì„¸ì…˜ ìƒíƒœë¥¼ í™œìš©í•˜ì—¬ ê·¸ë˜í”„ë¥¼ ì¬ì‚¬ìš©í•©ë‹ˆë‹¤.

    Args:
        query (str): ì‚¬ìš©ìê°€ ì…ë ¥í•œ ìì—°ì–´ ê¸°ë°˜ ì§ˆë¬¸.
        database_env (str): ì‚¬ìš©í•  ë°ì´í„°ë² ì´ìŠ¤ í™˜ê²½ ì´ë¦„ ë˜ëŠ” í‚¤ (ì˜ˆ: "dev", "prod").
        retriever_name (str, optional): í…Œì´ë¸” ê²€ìƒ‰ê¸° ì´ë¦„. ê¸°ë³¸ê°’ì€ "ê¸°ë³¸".
        top_n (int, optional): ê²€ìƒ‰ëœ ìƒìœ„ í…Œì´ë¸” ìˆ˜ ì œí•œ. ê¸°ë³¸ê°’ì€ 5.
        device (str, optional): LLM ì‹¤í–‰ì— ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ("cpu" ë˜ëŠ” "cuda"). ê¸°ë³¸ê°’ì€ "cpu".

    Returns:
        dict: ë‹¤ìŒ ì •ë³´ë¥¼ í¬í•¨í•œ Lang2SQL ì‹¤í–‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬:
            - "generated_query": ìƒì„±ëœ SQL ì¿¼ë¦¬ (`AIMessage`)
            - "messages": ì „ì²´ LLM ì‘ë‹µ ë©”ì‹œì§€ ëª©ë¡
            - "searched_tables": ì°¸ì¡°ëœ í…Œì´ë¸” ëª©ë¡ ë“± ì¶”ê°€ ì •ë³´
    """

    return execute_query_common(
        query=query,
        database_env=database_env,
        retriever_name=retriever_name,
        top_n=top_n,
        device=device,
        use_enriched_graph=st.session_state.get("use_enriched", True),
        session_state=st.session_state,
    )


def display_result(
    *,
    res: dict,
    database,
) -> None:
    """
    Lang2SQL ì‹¤í–‰ ê²°ê³¼ë¥¼ Streamlit í™”ë©´ì— ì¶œë ¥í•©ë‹ˆë‹¤.

    Args:
        res (dict): Lang2SQL ì‹¤í–‰ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬.
        database (ConnectDB): SQL ì¿¼ë¦¬ ì‹¤í–‰ì„ ìœ„í•œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ê°ì²´.

    ì¶œë ¥ í•­ëª©:
        - ì´ í† í° ì‚¬ìš©ëŸ‰
        - ìƒì„±ëœ SQL ì¿¼ë¦¬
        - ê²°ê³¼ ì„¤ëª…
        - AIê°€ ì¬í•´ì„í•œ ì‚¬ìš©ì ì§ˆë¬¸
        - ì°¸ì¡°ëœ í…Œì´ë¸” ëª©ë¡
        - ì¿¼ë¦¬ ì‹¤í–‰ ê²°ê³¼ í…Œì´ë¸”
    """

    def should_show(_key: str) -> bool:
        return st.session_state.get(_key, True)

    has_query = bool(res.get("generated_query"))
    # ì„¹ì…˜ í‘œì‹œ ì—¬ë¶€ë¥¼ QUERY_MAKER ì¶œë ¥ ìœ ë¬´ì— ë”°ë¼ ì œì–´
    show_sql_section = has_query and should_show("show_sql")
    show_result_desc = has_query and should_show("show_result_description")
    show_reinterpreted = has_query and should_show("show_question_reinterpreted_by_ai")
    show_table_section = has_query and should_show("show_table")
    show_chart_section = has_query and should_show("show_chart")

    if should_show("show_token_usage"):
        st.markdown("---")
        token_summary = TokenUtils.get_token_usage_summary(data=res["messages"])
        st.write("**í† í° ì‚¬ìš©ëŸ‰:**")
        st.markdown(
            f"""
        - Input tokens: `{token_summary['input_tokens']}`
        - Output tokens: `{token_summary['output_tokens']}`
        - Total tokens: `{token_summary['total_tokens']}`
        """
        )

    if show_sql_section:
        st.markdown("---")
        generated_query = res.get("generated_query")
        if generated_query:
            query_text = (
                generated_query.content
                if isinstance(generated_query, AIMessage)
                else str(generated_query)
            )

            # query_textê°€ ë¬¸ìì—´ì¸ì§€ í™•ì¸
            if isinstance(query_text, str):
                try:
                    sql = LLMResponseParser.extract_sql(query_text)
                    st.markdown("**ìƒì„±ëœ SQL ì¿¼ë¦¬:**")
                    st.code(sql, language="sql")
                except ValueError:
                    st.warning("SQL ë¸”ë¡ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    st.text(query_text)

                interpretation = LLMResponseParser.extract_interpretation(query_text)
                if interpretation:
                    st.markdown("**ê²°ê³¼ í•´ì„:**")
                    st.code(interpretation)
            else:
                st.warning("ì¿¼ë¦¬ í…ìŠ¤íŠ¸ê°€ ë¬¸ìì—´ì´ ì•„ë‹™ë‹ˆë‹¤.")
                st.text(str(query_text))

    if show_result_desc:
        st.markdown("---")
        st.markdown("**ê²°ê³¼ ì„¤ëª…:**")
        result_message = res["messages"][-1].content

        if isinstance(result_message, str):
            try:
                sql = LLMResponseParser.extract_sql(result_message)
                st.code(sql, language="sql")
            except ValueError:
                st.warning("SQL ë¸”ë¡ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                st.text(result_message)

            interpretation = LLMResponseParser.extract_interpretation(result_message)
            if interpretation:
                st.code(interpretation, language="plaintext")
        else:
            st.warning("ê²°ê³¼ ë©”ì‹œì§€ê°€ ë¬¸ìì—´ì´ ì•„ë‹™ë‹ˆë‹¤.")
            st.text(str(result_message))

    if show_reinterpreted:
        st.markdown("---")
        st.markdown("**AIê°€ ì¬í•´ì„í•œ ì‚¬ìš©ì ì§ˆë¬¸:**")
        try:
            if len(res["messages"]) > 1:
                candidate = res["messages"][-2]
                question_text = (
                    candidate.content
                    if hasattr(candidate, "content")
                    else str(candidate)
                )
            else:
                question_text = res["messages"][0].content
        except Exception:
            question_text = str(res["messages"][0].content)
        st.code(question_text)

    if should_show("show_referenced_tables"):
        st.markdown("---")
        st.markdown("**ì°¸ê³ í•œ í…Œì´ë¸” ëª©ë¡:**")
        st.write(res.get("searched_tables", []))

    # QUERY_MAKERê°€ ë¹„í™œì„±í™”ëœ ê²½ìš° ì•ˆë‚´ ë©”ì‹œì§€ ì¶œë ¥
    if not has_query:
        st.info("QUERY_MAKER ì—†ì´ ì‹¤í–‰ë˜ì—ˆìŠµë‹ˆë‹¤. ê²€ìƒ‰ëœ í…Œì´ë¸” ì •ë³´ë§Œ í‘œì‹œí•©ë‹ˆë‹¤.")

    if show_table_section:
        st.markdown("---")
        try:
            sql_raw = (
                res["generated_query"].content
                if isinstance(res["generated_query"], AIMessage)
                else str(res["generated_query"])
            )
            if isinstance(sql_raw, str):
                sql = LLMResponseParser.extract_sql(sql_raw)
                df = database.run_sql(sql)
                st.dataframe(df.head(10) if len(df) > 10 else df)
            else:
                st.error("SQL ì›ë³¸ì´ ë¬¸ìì—´ì´ ì•„ë‹™ë‹ˆë‹¤.")
        except Exception as e:
            st.error(f"ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    if show_chart_section:
        st.markdown("---")
        try:
            sql_raw = (
                res["generated_query"].content
                if isinstance(res["generated_query"], AIMessage)
                else str(res["generated_query"])
            )
            if isinstance(sql_raw, str):
                sql = LLMResponseParser.extract_sql(sql_raw)
                df = database.run_sql(sql)
                st.markdown("**ì¿¼ë¦¬ ê²°ê³¼ ì‹œê°í™”:**")
                try:
                    if len(res["messages"]) > 1:
                        candidate = res["messages"][-2]
                        chart_question = (
                            candidate.content
                            if hasattr(candidate, "content")
                            else str(candidate)
                        )
                    else:
                        chart_question = res["messages"][0].content
                except Exception:
                    chart_question = str(res["messages"][0].content)

                display_code = DisplayChart(
                    question=chart_question,
                    sql=sql,
                    df_metadata=f"Running df.dtypes gives:\n{df.dtypes}",
                )
                # plotly_code ë³€ìˆ˜ë„ ë”°ë¡œ ë³´ê´€í•  í•„ìš” ì—†ì´ ë°”ë¡œ ê·¸ë ¤ë„ ë©ë‹ˆë‹¤
                fig = display_code.get_plotly_figure(
                    plotly_code=display_code.generate_plotly_code(), df=df
                )
                st.plotly_chart(fig)
            else:
                st.error("SQL ì›ë³¸ì´ ë¬¸ìì—´ì´ ì•„ë‹™ë‹ˆë‹¤.")
        except Exception as e:
            st.error(f"ì°¨íŠ¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


# ì„ì‹œë¡œ ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë¹„í™œì„±í™” (FAISS í…ŒìŠ¤íŠ¸ìš©)
# db = get_db_connector()
db = None

st.title(TITLE)

# ì›Œí¬í”Œë¡œìš° ì„ íƒ(UI)
# st.sidebar.markdown("### ì›Œí¬í”Œë¡œìš° ì„ íƒ")
# use_enriched = st.sidebar.checkbox(
#     "ğŸš€ í™•ì¥ëœ ì›Œí¬í”Œë¡œìš° (í”„ë¡œíŒŒì¼ ì¶”ì¶œ & ì»¨í…ìŠ¤íŠ¸ ë³´ê°•)", 
#     value=True,
#     help="ë” ì •í™•í•œ SQL ìƒì„±ì„ ìœ„í•´ ì§ˆë¬¸ì„ ë¶„ì„í•˜ê³  ì»¨í…ìŠ¤íŠ¸ë¥¼ ë³´ê°•í•©ë‹ˆë‹¤. (ê¶Œì¥)"
# )
use_enriched = True

# ëª¨ë‹ˆí„°ë§ ì˜µì…˜
# st.sidebar.markdown("### ëª¨ë‹ˆí„°ë§ ì˜µì…˜")
# enable_monitoring = st.sidebar.checkbox(
#     "ğŸ” ì‹¤ì‹œê°„ ë…¸ë“œ ì‹¤í–‰ ëª¨ë‹ˆí„°ë§", 
#     value=True,
#     help="ê° LangGraph ë…¸ë“œì˜ ì‹¤í–‰ ê³¼ì •ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•©ë‹ˆë‹¤."
# )
enable_monitoring = True

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "graph" not in st.session_state or st.session_state.get("use_enriched") != use_enriched:
    # í™•ì¥ëœ ê·¸ë˜í”„ë¡œ ê³ ì •
    graph_builder = enriched_builder
    st.session_state["graph"] = graph_builder.compile()
    st.session_state["use_enriched"] = use_enriched

# # ìƒˆë¡œê³ ì¹¨ ë²„íŠ¼ ì¶”ê°€
# if st.sidebar.button("Lang2SQL ìƒˆë¡œê³ ì¹¨"):
#     # ê·¸ë˜í”„ ì„ íƒ ë¡œì§
#     if st.session_state.get("use_enriched"):
#         graph_builder = enriched_builder
#         graph_type = "í™•ì¥ëœ"
#     else:
#         graph_builder = builder
#         graph_type = "ê¸°ë³¸"

#     st.session_state["graph"] = graph_builder.compile()
#     st.sidebar.success(
#         f"Lang2SQLì´ ì„±ê³µì ìœ¼ë¡œ ìƒˆë¡œê³ ì¹¨ë˜ì—ˆìŠµë‹ˆë‹¤. ({graph_type} ì›Œí¬í”Œë¡œìš°)"
#     )

user_query = st.text_area(
    "ì¿¼ë¦¬ë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
    value=DEFAULT_QUERY,
)
# user_database_env = st.selectbox(
#     "DB í™˜ê²½ì •ë³´ë¥¼ ì…ë ¥í•˜ì„¸ìš”:",
#     options=SQL_PROMPTS.keys(),
#     index=0,
# )
user_database_env = "create"

# _device_options = ["cpu", "cuda"]
# _default_device = st.session_state.get("default_device", "cpu")
# _device_index = (
#     _device_options.index(_default_device) if _default_device in _device_options else 0
# )
# device = st.selectbox(
#     "ëª¨ë¸ ì‹¤í–‰ ì¥ì¹˜ë¥¼ ì„ íƒí•˜ì„¸ìš”:",
#     options=_device_options,
#     index=_device_index,
# )
device = "cpu"

retriever_options = {
    "ê¸°ë³¸": "ë²¡í„° ê²€ìƒ‰ (ê¸°ë³¸)",
    "Reranker": "Reranker ê²€ìƒ‰ (ì •í™•ë„ í–¥ìƒ)",
}

_retriever_keys = list(retriever_options.keys())
_default_retriever = st.session_state.get("default_retriever_name", "ê¸°ë³¸")
_retriever_index = (
    _retriever_keys.index(_default_retriever)
    if _default_retriever in _retriever_keys
    else 0
)
user_retriever = st.selectbox(
    "ê²€ìƒ‰ê¸° ìœ í˜•ì„ ì„ íƒí•˜ì„¸ìš”:",
    options=_retriever_keys,
    format_func=lambda x: retriever_options[x],
    index=_retriever_index,
)

user_top_n = st.slider(
    "ê²€ìƒ‰í•  í…Œì´ë¸” ì •ë³´ ê°œìˆ˜:",
    min_value=1,
    max_value=20,
    value=int(st.session_state.get("default_top_n", 5)),
    step=1,
    help="ê²€ìƒ‰í•  í…Œì´ë¸” ì •ë³´ì˜ ê°œìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤. ê°’ì´ í´ìˆ˜ë¡ ë” ë§ì€ í…Œì´ë¸” ì •ë³´ë¥¼ ê²€ìƒ‰í•˜ì§€ë§Œ ì²˜ë¦¬ ì‹œê°„ì´ ê¸¸ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
)

# st.sidebar.title("Output Settings")
# for key, label in SIDEBAR_OPTIONS.items():
#     st.sidebar.checkbox(label, value=True, key=key)

if st.button("ì¿¼ë¦¬ ì‹¤í–‰"):
    
    if enable_monitoring:
        st.markdown("---")
        st.subheader("ğŸ”„ LangGraph ë…¸ë“œ ì‹¤í–‰ ëª¨ë‹ˆí„°ë§")
        st.info("ê° ë…¸ë“œì˜ ì‹¤í–‰ ê³¼ì •ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ í™•ì¸í•˜ì„¸ìš”!")
        
        result = execute_query_with_monitoring(
            query=user_query,
            database_env=user_database_env,
            retriever_name=user_retriever,
            top_n=user_top_n,
            device=device,
        )
        
        st.markdown("---")
        st.subheader("âœ… ì‹¤í–‰ ì™„ë£Œ")
        
    else:
        result = execute_query(
            query=user_query,
            database_env=user_database_env,
            retriever_name=user_retriever,
            top_n=user_top_n,
            device=device,
        )
    
    # ë°ì´í„°ë² ì´ìŠ¤ê°€ ì—°ê²°ë˜ì§€ ì•Šì€ ê²½ìš° SQLë§Œ í‘œì‹œ
    if db is None:
        st.subheader("ğŸ” ìƒì„±ëœ SQL ì¿¼ë¦¬")
        generated_query = result.get("generated_query")
        if generated_query:
            query_text = (
                generated_query.content
                if hasattr(generated_query, "content")
                else str(generated_query)
            )
            st.code(query_text, language="sql")
        st.info("ğŸ’¡ ì‹¤ì œ ë°ì´í„°ë² ì´ìŠ¤ì— ì—°ê²°í•˜ë©´ ì¿¼ë¦¬ ê²°ê³¼ì™€ ì°¨íŠ¸ë¥¼ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        display_result(res=result, database=db)
