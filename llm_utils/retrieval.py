import os
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import CrossEncoderReranker
from langchain_community.cross_encoders import HuggingFaceCrossEncoder
from transformers import AutoModelForSequenceClassification, AutoTokenizer

from llm_utils.vectordb import get_vector_db


def load_reranker_model(device: str = "cpu"):
    """í•œêµ­ì–´ reranker ëª¨ë¸ì„ ë¡œë“œí•˜ê±°ë‚˜ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."""
    local_model_path = os.path.join(os.getcwd(), "ko_reranker_local")

    # ë¡œì»¬ì— ì €ì¥ëœ ëª¨ë¸ì´ ìˆìœ¼ë©´ ë¶ˆëŸ¬ì˜¤ê³ , ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ í›„ ì €ì¥
    if os.path.exists(local_model_path) and os.path.isdir(local_model_path):
        print("ğŸ”„ ko-reranker ëª¨ë¸ ë¡œì»¬ì—ì„œ ë¡œë“œ ì¤‘...")
    else:
        print("â¬‡ï¸ ko-reranker ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ë° ì €ì¥ ì¤‘...")
        model = AutoModelForSequenceClassification.from_pretrained(
            "Dongjin-kr/ko-reranker"
        )
        tokenizer = AutoTokenizer.from_pretrained("Dongjin-kr/ko-reranker")
        model.save_pretrained(local_model_path)
        tokenizer.save_pretrained(local_model_path)

    return HuggingFaceCrossEncoder(
        model_name=local_model_path,
        model_kwargs={"device": device},
    )


def get_retriever(retriever_name: str = "ê¸°ë³¸", top_n: int = 5, device: str = "cpu"):
    """ê²€ìƒ‰ê¸° íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ ê²€ìƒ‰ê¸°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Args:
        retriever_name: ì‚¬ìš©í•  ê²€ìƒ‰ê¸° ì´ë¦„ ("ê¸°ë³¸", "ì¬ìˆœìœ„", ë“±)
        top_n: ë°˜í™˜í•  ìƒìœ„ ê²°ê³¼ ê°œìˆ˜
    """
    print(device)
    retrievers = {
        "ê¸°ë³¸": lambda: get_vector_db().as_retriever(search_kwargs={"k": top_n}),
        "Reranker": lambda: ContextualCompressionRetriever(
            base_compressor=CrossEncoderReranker(
                model=load_reranker_model(device), top_n=top_n
            ),
            base_retriever=get_vector_db().as_retriever(search_kwargs={"k": top_n}),
        ),
    }

    if retriever_name not in retrievers:
        print(
            f"ê²½ê³ : '{retriever_name}' ê²€ìƒ‰ê¸°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ ê²€ìƒ‰ê¸°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
        )
        retriever_name = "ê¸°ë³¸"

    return retrievers[retriever_name]()


def search_tables(
    query: str, retriever_name: str = "ê¸°ë³¸", top_n: int = 5, device: str = "cpu"
):
    """ì¿¼ë¦¬ì— ë§ëŠ” í…Œì´ë¸” ì •ë³´ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    print(f"ğŸ” ê²€ìƒ‰ ì‹œì‘: '{query}' (retriever: {retriever_name}, top_n: {top_n})")
    
    try:
        if retriever_name == "ê¸°ë³¸":
            db = get_vector_db()
            print(f"âœ… ë²¡í„° DB ë¡œë“œ ì„±ê³µ")
            
            # similarity_search_with_scoreë¥¼ ì‚¬ìš©í•˜ì—¬ ìœ ì‚¬ë„ ì ìˆ˜ë„ í•¨ê»˜ ê°€ì ¸ì˜´
            doc_score_pairs = db.similarity_search_with_score(query, k=top_n)
            doc_res = [(doc, score) for doc, score in doc_score_pairs]
            print(f"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼: {len(doc_res)}ê°œ ë¬¸ì„œ ì°¾ìŒ")
        else:
            retriever = get_retriever(
                retriever_name=retriever_name, top_n=top_n, device=device
            )
            docs = retriever.invoke(query)
            # Rerankerì˜ ê²½ìš° score ì •ë³´ê°€ ì—†ìœ¼ë¯€ë¡œ ê¸°ë³¸ê°’ ì„¤ì •
            doc_res = [(doc, getattr(doc, 'metadata', {}).get('score', 0.5)) for doc in docs]
            print(f"ğŸ“Š Reranker ê²€ìƒ‰ ê²°ê³¼: {len(doc_res)}ê°œ ë¬¸ì„œ ì°¾ìŒ")

        # ê²°ê³¼ë¥¼ ì‚¬ì „ í˜•íƒœë¡œ ë³€í™˜
        documents_dict = {}
        for i, (doc, score) in enumerate(doc_res):
            try:
                lines = doc.page_content.split("\n")
                print(f"ğŸ“„ ì²˜ë¦¬ ì¤‘ì¸ ë¬¸ì„œ {i+1}: {lines[0][:50]}...")

                # í…Œì´ë¸”ëª… ë° ì„¤ëª… ì¶”ì¶œ
                if ": " not in lines[0]:
                    print(f"âš ï¸ ê²½ê³ : í…Œì´ë¸” ì •ë³´ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤: {lines[0]}")
                    continue
                    
                table_name, table_desc = lines[0].split(": ", 1)

                # ì„¹ì…˜ë³„ë¡œ ì •ë³´ ì¶”ì¶œ (í…Œì´ë¸”/ì»¬ëŸ¼ë§Œ ì‚¬ìš©)
                columns = {}
                current_section = None

                for j, line in enumerate(lines[1:], 1):
                    line = line.strip()

                    # ì„¹ì…˜ í—¤ë” í™•ì¸
                    if line == "Columns:":
                        current_section = "columns"
                        continue

                    # ê° ì„¹ì…˜ì˜ ë‚´ìš© íŒŒì‹±
                    if current_section == "columns" and ": " in line:
                        col_name, col_desc = line.split(": ", 1)
                        columns[col_name.strip()] = col_desc.strip()

                # ë”•ì…”ë„ˆë¦¬ ì €ì¥ (ìœ ì‚¬ë„ ì ìˆ˜ í¬í•¨)
                documents_dict[table_name] = {
                    "table_description": table_desc.strip(),
                    "score": f"{score:.3f}",  # ìœ ì‚¬ë„ ì ìˆ˜ ì¶”ê°€
                    "rank": i + 1,  # ìˆœìœ„ ì¶”ê°€
                    **columns,  # ì»¬ëŸ¼ ì •ë³´ ì¶”ê°€
                }
                print(f"âœ… í…Œì´ë¸” '{table_name}' ì²˜ë¦¬ ì™„ë£Œ (ìœ ì‚¬ë„: {score:.3f})")
                
            except Exception as e:
                print(f"âŒ ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                print(f"ë¬¸ì œê°€ ëœ ë¬¸ì„œ: {doc.page_content[:100]}...")
                continue

        print(f"ğŸ¯ ìµœì¢… ê²°ê³¼: {len(documents_dict)}ê°œ í…Œì´ë¸” ë°˜í™˜")
        return documents_dict
        
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ ì¤‘ ì „ì²´ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
        import traceback
        traceback.print_exc()
        return {}
